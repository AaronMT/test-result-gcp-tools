name: Batch ingest JUnit XML reports from Cloud Storage to Sheets

on:
    workflow_dispatch:
    schedule:
        - cron: '0 5 * * *'  # Runs at 5:00 AM UTC every day

jobs:
    ingest_reports:
        name: Ingest JUnit Reports for ${{ matrix.project.name }}
        runs-on: ubuntu-latest
        strategy:
          matrix:
              project:
                  - name: "Fenix"
                    bucket_name: "GCS_BUCKET_NAME_A"
                  - name: "Focus"
                    bucket_name: "GCS_BUCKET_NAME_B"
        steps:
            - name: Checkout the repository
              uses: actions/checkout@v6.0.1
            - name: Authenticate with Google Cloud
              uses: google-github-actions/auth@v3.0.0
              with:
                credentials_json: ${{ secrets.GCP_SA_KEY }}
            - name: Set up Google Cloud SDK
              uses: google-github-actions/setup-gcloud@v3.0.1
              with:
                project_id: ${{ secrets.GCP_PROJECT_ID }}
            - name: Set up Python 3.
              uses: actions/setup-python@v6.1.0
              with:
                python-version: '3.12'
            - name: Enable caching
              uses: astral-sh/setup-uv@v7
              with:
                enable-cache: true
            - name: Install Dependencies
              run: |
                uv pip install --system junitparser==4.0.2
                uv pip install --system gspread==6.2.1
            - name: Copy JUnit reports from the last 24 hours from GCS
              run: |
                BUCKET_NAME="${{ secrets[matrix.project.bucket_name] }}"
                DATE_PREFIX=$(date -u -d 'yesterday' +%Y-%m-%d)  # Get the current date in UTC

                mkdir -p junit_reports

                # Find and download the FullJunitXmlReport.xml files from the last 24 hours
                gsutil ls "gs://$BUCKET_NAME/*$DATE_PREFIX*/FullJUnitReport.xml" | while read -r file; do
                  # Extract directory name from the full path
                  DIR_NAME=$(basename $(dirname "$file"))
                  # Define the destination file name with the directory name appended
                  DEST_FILE="./junit_reports/FullJUnitReport-${DIR_NAME}.xml"
                  # Matrix file path
                  MATRIX_FILE="$(dirname "$file")/matrix_ids.json"
                  LOCAL_MATRIX_FILE="./junit_reports/matrix_ids-${DIR_NAME}.json"

                  if gsutil cp "$MATRIX_FILE" "$LOCAL_MATRIX_FILE"; then
                    echo "Inspecting $LOCAL_MATRIX_FILE"
                    if [ -f "$LOCAL_MATRIX_FILE" ]; then
                      MATRIX_LABEL=$(jq -r 'to_entries | map(select(.value.clientDetails.matrixLabel == "try")) | first | .value.clientDetails.matrixLabel' "$LOCAL_MATRIX_FILE")
                      if [ "$MATRIX_LABEL" == "try" ]; then
                        echo "Skipping $file because matrixLabel is 'try'"
                        continue
                      fi
                    fi
                  else
                    echo "Warning: matrix_ids.json not found for $file, proceeding with copy"
                  fi

                  # Copy the FullJUnitReport.xml file with the new name to avoid overwriting
                  gsutil cp "$file" "$DEST_FILE"
                done
            - name: Inspect and remove empty JUnit XML reports
              run: |
                python scripts/src/inspect_reports.py junit_reports
            - name: Archive the reports into a zip file
              run: |
                ZIP_FILE="FullJunitXmlReports_$(date +%Y%m%d_%H%M%S).zip"
                zip -r $ZIP_FILE junit_reports/
                echo "ZIP_FILE=$ZIP_FILE" >> $GITHUB_ENV
            - name: Run aggregation script
              env:
                GOOGLE_SHEETS_KEY: ${{ secrets.GCP_SA_KEY}}
                PROJECT_NAME: ${{ matrix.project.name }}
              run: |
                python scripts/src/ingest_spreadsheet.py junit_reports
            - name: Upload reports artifact
              uses: actions/upload-artifact@v6.0.0
              with:
                name: junit-xml-reports-${{ matrix.project.name }}
                path: ${{ env.ZIP_FILE }}
            - name: Upload CSV artifacts
              uses: actions/upload-artifact@v6.0.0
              with:
                name: junit-xml-reports-${{ matrix.project.name }}-csv
                path: |
                  aggregated_test_results.csv
                  daily_totals.csv
            - name: Convert CSV percentages to Floats
              if: success()  # Ensure the previous steps completed successfully
              run: |
                # Ensure the file exists before attempting conversion
                if [[ -f "daily_totals.csv" ]]; then
                  echo "Converting percentage values in daily_totals.csv to float format..."
                  
                  awk -F',' 'BEGIN {OFS=","} NR==1 {print $0} NR>1 {
                    $5=sprintf("%.6f", substr($5, 1, length($5)-1)/100);
                    $6=sprintf("%.6f", substr($6, 1, length($6)-1)/100);
                    print $0
                  }' daily_totals.csv > daily_totals_tmp.csv

                  mv daily_totals_tmp.csv daily_totals.csv
                  echo "Conversion complete."
                else
                  echo "daily_totals.csv not found, skipping conversion."
                fi
              shell: bash
            - name: Upload daily_totals.csv to BigQuery (staging + MERGE upsert)
              if: success()
              run: |
                set -euo pipefail

                gcloud config set project ${{ secrets.BQ_PROJECT_ID }}

                declare -A prod_tables
                prod_tables["Fenix"]="testops_stats.fenix_daily_android"
                prod_tables["Focus"]="testops_stats.focus_daily_android"

                declare -A staging_tables
                staging_tables["Fenix"]="testops_stats._staging_fenix_daily_android"
                staging_tables["Focus"]="testops_stats._staging_focus_daily_android"

                prod_table="${prod_tables[${{ matrix.project.name }}]}"
                staging_table="${staging_tables[${{ matrix.project.name }}]}"
                csv_file="daily_totals.csv"

                if [[ ! -f "$csv_file" ]]; then
                  echo "No CSV file found for ${{ matrix.project.name }}. Skipping."
                  exit 0
                fi

                echo "Project: ${{ matrix.project.name }}"
                echo "Staging: $staging_table"
                echo "Prod:    $prod_table"
                echo "CSV:     $csv_file"

                # Safety check: refuse to run bq load --replace on non-staging table
                if [[ "$staging_table" != testops_stats._staging_* ]]; then
                  echo "ERROR: Refusing to run bq load --replace on non-staging table: $staging_table"
                  exit 1
                fi

                # 1) Load into staging (overwrite staging each run; skip header row)
                bq load \
                  --project_id=${{ secrets.BQ_PROJECT_ID }} \
                  --schema=.github/schemas/daily_totals_schema.json \
                  --source_format=CSV \
                  --skip_leading_rows=1 \
                  --replace \
                  "$staging_table" \
                  "$csv_file"

                # 2) MERGE staging -> prod (idempotent on Date)
                bq --project_id=${{ secrets.BQ_PROJECT_ID }} query --use_legacy_sql=false \
                "MERGE \`${{ secrets.BQ_PROJECT_ID }}.${prod_table}\` T
                 USING \`${{ secrets.BQ_PROJECT_ID }}.${staging_table}\` S
                 ON T.Date = S.Date
                 WHEN MATCHED THEN UPDATE SET
                   \`Total Runs\`   = S.\`Total Runs\`,
                   \`Flaky Runs\`   = S.\`Flaky Runs\`,
                   \`Failed Runs\`  = S.\`Failed Runs\`,
                   \`Flaky Rate\`   = S.\`Flaky Rate\`,
                   \`Failure Rate\` = S.\`Failure Rate\`
                 WHEN NOT MATCHED THEN INSERT (
                   Date, \`Total Runs\`, \`Flaky Runs\`, \`Failed Runs\`, \`Flaky Rate\`, \`Failure Rate\`
                 ) VALUES (
                   S.Date, S.\`Total Runs\`, S.\`Flaky Runs\`, S.\`Failed Runs\`, S.\`Flaky Rate\`, S.\`Failure Rate\`
                 );"

                # 3) Guardrail: fail if duplicates exist
                echo "Checking for duplicate Dates in prod table..."
                dup_count=$(bq --project_id=${{ secrets.BQ_PROJECT_ID }} query --use_legacy_sql=false --format=csv \
                  "SELECT COUNT(1)
                   FROM (
                     SELECT Date
                     FROM \`${{ secrets.BQ_PROJECT_ID }}.${prod_table}\`
                     GROUP BY Date
                     HAVING COUNT(*) > 1
                   )" | tail -n 1)

                if [[ ${dup_count} -ne 0 ]]; then
                  echo "ERROR: Duplicate Dates detected in ${prod_table} (count=${dup_count})."
                  exit 1
                fi

                echo "Done. No duplicates detected."
              shell: bash
